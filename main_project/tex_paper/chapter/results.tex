% Results

C programs from the course were used to estimate MHC binding affinity with PSSM.
Here a threshold must be used to limit included peptides to the ones that actually bind to the MHC molecule. 
In the machine learning methods non-binding data is also be valuable in the estimation.

All entries from the training data set with binding affinity over 0.426 are feeded into a program. 
This value means that the peptide successfully binds to the MHC molecule. All peptides should be of same length.
The Blosum frequency substitution matrix, is a
conditional probability matrix of matching amino
acids j given you have amino acid i

\begin{equation}
P(j|i) = \frac{P_ij}{Q_i}
\end{equation}

The matrix is used to. The result is a PSSM matrix. 

Experimental data from the HLA-A*0201 allele is randomly split up in training and evaluation sections, the evaluation section being 1/5 of the training data size (618 peptides vs 2471 peptides in the training set).
For 5 different samplings we get a Pearson correlation coefficients of {0.75323,, 0.78415, 0.78009, 0.78896, 0.76480}, the average being 0.774.
The corresponding results without using sequence weighting are {0.73530, 0.75961, 0.75791, 0.76343, 0.74639}, with average of 0.753
Using identity matrix instead of the blosum substitiution matrix gives similar values {0.75107, 0.77989, 0.74344, 0.78552, 0.76072} with average of 0.764.
The blosum matrix is not making a difference here as substitution frequencies should not affect much binding affinity of a peptide in a MHC molecule.

We also try the smaller set HLA-A3001. It has a total of 669 experimentally verified peptides.
Using the blosum matrix the correlation results are {0.69872, 0.55787, 0.65032, 0.55827, 0.57044}, an average of 0.607
The corresponding results without using sequence weighting are {0.67775, 0.54853, 0.63731, 0.55026, 0.56416}, with average of 0.596
The identity matrix gives results of with average of (0.63987 0.45267 0.55898 0.49296 0.49224) 0.527 

Using a small dataset the benefit of using a blosum frequency substitution matrix is greater.

\subsection{SVM}
There exists a fairly well established programming library for SVM calculations, libsvm, but in this investigation version 3.6.3 of the java based Weka sofware is used. 
The data has to be prepared for input into the Weka program. 
The number of amino acids is 20 and the length of the peptide is 9 so each peptide is changed to a vector of length 20*9. 
Three encoding schemes are used, blostum50, sparse and z-score. 
In the case of the blosum encoding scheme, each of the 9 sections of length 20 simply contain the data from each row in blosum matrix corresponding to the amino acid in question.
The sparse encoding simply uses the identity substitution matrix similar to the one in the PSSM case. 
The z-score encoding is a condensed manner to represent each amino acid, only 5 values are used to encode each amino acid based on on their properties. 
These properties were deduced from measured data using thin-layer chromatography and nuclear magnetic resonance and some calculated variables 
such as side chain charge, hydrogren bond donor and acceptor properties, log P and molecular weight. 
Each row in the input matrix has therefore length of 5*20+1 (the one being the measured affinity value)
The same data as from the PSSM section is used for comparison. For the HLA-A*0201 dataset and sparse encoding we get the following results


\begin{tabular}{ c r }
Correlation coefficient &                 0.78  \\
Mean absolute error      &                0.1524 \\
Root mean squared error   &               0.1925 \\
Relative absolute error    &             57.0908\%  \\
Root relative squared error &            62.8014\%  \\
Total Number of Instances    &          618 \\
\end{tabular}


Here the SMOreg classifier with polynomial kernel of first degree we get just slightly better than the PSSM. Raising the degree of the kernel to 2 does not give better results

\begin{tabular}{ c r }
Correlation coefficient &                 0.756 \\
Mean absolute error      &                0.1535 \\
Root mean squared error   &               0.2039 \\
Relative absolute error   &              57.5209\% \\
Root relative squared error &            66.4967\% \\
Total Number of Instances    &          618 \\
\end{tabular}

Using the blosum matrix with kernel degree of 1
\begin{tabular}{ c r }
Correlation coefficient  &                0.7789 \\
Mean absolute error       &               0.1533 \\
Root mean squared error   &               0.1928 \\
Relative absolute error   &              57.4187\% \\
Root relative squared error  &           62.8817\% \\
Total Number of Instances     &         618  \\
\end{tabular}

Polynomial kernel of degree 2 has considerably worse performance for the blosum encoded data.
\begin{tabular}{ c r }	
Correlation coefficient                  0.6692 \\
Mean absolute error                      0.2047 \\
Root mean squared error                  0.262 \\
Relative absolute error                 76.6914\% \\
Root relative squared error             85.4726\% \\
Total Number of Instances              618  \\
\end{tabular}{ c r }

Now we try the z-score encoded data with polynomial kernel of degree 1
\begin{tabular}{ c r }
Correlation coefficient   &              0.6888 \\
Mean absolute error        &              0.1802 \\
Root mean squared error    &              0.2227 \\
Relative absolute error    &             67.5222\% \\
Root relative squared error &             72.6292\% \\
Total Number of Instances   &           618     \\
\end{tabular}

Next we try a smaller dataset, the HLA-A*3001.

\begin{tabular}{ c r }
Correlation coefficient       &           0.7412 \\
Mean absolute error            &          0.1008 \\
Root mean squared error        &          0.1473 \\
Relative absolute error        &         70.3882\% \\
Root relative squared error    &         68.5173\% \\
Total Number of Instances      &        134 \\
\end{tabular}

Polynomial kernel of degree 2 slightly raises the prediction performance.

\begin{tabular}{ c r }
Correlation coefficient        &          0.7671 \\
Mean absolute error            &          0.0945 \\
Root mean squared error        &          0.1424 \\
Relative absolute error        &         65.9762\% \\
Root relative squared error    &         66.2741\% \\
Total Number of Instances      &        134 \\
\end{tabular}
